{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ad211de-ece6-4ab2-990d-0ace67edc918",
      "metadata": {
        "id": "2ad211de-ece6-4ab2-990d-0ace67edc918",
        "outputId": "90c5b01e-634a-4102-db00-c4f918cd6891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: resampy in c:\\users\\curso\\anaconda3\\envs\\miar_rl\\lib\\site-packages (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\curso\\anaconda3\\envs\\miar_rl\\lib\\site-packages (from resampy) (1.24.4)\n",
            "Requirement already satisfied: numba>=0.53 in c:\\users\\curso\\anaconda3\\envs\\miar_rl\\lib\\site-packages (from resampy) (0.58.1)\n",
            "Requirement already satisfied: importlib-resources in c:\\users\\curso\\anaconda3\\envs\\miar_rl\\lib\\site-packages (from resampy) (6.4.5)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\curso\\anaconda3\\envs\\miar_rl\\lib\\site-packages (from numba>=0.53->resampy) (0.41.1)\n",
            "Requirement already satisfied: importlib-metadata in c:\\users\\curso\\anaconda3\\envs\\miar_rl\\lib\\site-packages (from numba>=0.53->resampy) (8.5.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\curso\\anaconda3\\envs\\miar_rl\\lib\\site-packages (from importlib-resources->resampy) (3.20.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install resampy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
        "mount='/content/gdrive'\n",
        "drive_root = mount + \"/My Drive/TFM\"\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False"
      ],
      "metadata": {
        "id": "70Lw3EOe9NAP"
      },
      "id": "70Lw3EOe9NAP",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Switch to the directory on the Google Drive that you want to use\n",
        "import os\n",
        "if IN_COLAB:\n",
        "  print(\"We're running Colab\")\n",
        "\n",
        "  if IN_COLAB:\n",
        "    # Mount the Google Drive at mount\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "      os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "# Verify we're in the correct working directory\n",
        "%pwd\n",
        "print(\"Archivos en el directorio: \")\n",
        "print(os.listdir())\n",
        ""
      ],
      "metadata": {
        "id": "4VZTIZyv9o3N",
        "outputId": "f55c5c29-63bf-4407-cab5-bc209a06b2c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4VZTIZyv9o3N",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We're running Colab\n",
            "Colab: mounting Google drive on  /content/gdrive\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "\n",
            "Colab: making sure  /content/gdrive/My Drive/TFM  exists.\n",
            "\n",
            "Colab: Changing directory to  /content/gdrive/My Drive/TFM\n",
            "/content/gdrive/My Drive/TFM\n",
            "Archivos en el directorio: \n",
            "['Dataset_TFM.zip', 'Dataset_TFM_recortado.zip', 'TFM_V0.ipynb', 'TFM_V1.ipynb', 'Dataset']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilizando Data Augmentation para aumentar la muestra"
      ],
      "metadata": {
        "id": "5YT1B-iMCIpk"
      },
      "id": "5YT1B-iMCIpk"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install audiomentations"
      ],
      "metadata": {
        "id": "EHskuujPClzL",
        "outputId": "06d1af88-3db2-41d3-a21d-27aeacf68ea3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EHskuujPClzL",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: audiomentations in /usr/local/lib/python3.11/dist-packages (0.41.0)\n",
            "Requirement already satisfied: numpy<3,>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (2.0.2)\n",
            "Requirement already satisfied: numpy-minmax<1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (0.5.0)\n",
            "Requirement already satisfied: numpy-rms<1,>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (0.6.0)\n",
            "Requirement already satisfied: librosa!=0.10.0,<0.11.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (0.10.2.post1)\n",
            "Requirement already satisfied: python-stretch<1,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (0.3.1)\n",
            "Requirement already satisfied: scipy<2,>=1.4 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (1.15.3)\n",
            "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (0.5.0.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.14.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.1.1)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy-loader>=0.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch\n",
        "import soundfile as sf  # Para guardar archivos WAV\n",
        "\n",
        "# Configuración\n",
        "INPUT_FOLDER = \"Dataset\"\n",
        "OUTPUT_FOLDER = \"Dataset_Augmented\"\n",
        "AUGMENTATIONS_PER_FILE = 2  # Número de aumentos por audio\n",
        "\n",
        "# Transformaciones (personaliza parámetros)\n",
        "augmenter = Compose([\n",
        "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
        "    PitchShift(min_semitones=-3, max_semitones=3, p=0.7),\n",
        "    TimeStretch(min_rate=0.85, max_rate=1.15, p=0.5),\n",
        "])\n",
        "\n",
        "# Crear carpeta de salida si no existe\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# Procesar cada subcarpeta (emociones)\n",
        "for emotion in os.listdir(INPUT_FOLDER):\n",
        "    emotion_path = os.path.join(INPUT_FOLDER, emotion)\n",
        "    output_emotion_path = os.path.join(OUTPUT_FOLDER, emotion)\n",
        "    os.makedirs(output_emotion_path, exist_ok=True)\n",
        "\n",
        "    print(f\"Procesando: {emotion}...\")\n",
        "\n",
        "    for audio_file in os.listdir(emotion_path):\n",
        "        if not audio_file.endswith(\".wav\"):\n",
        "            continue\n",
        "\n",
        "        # Cargar audio\n",
        "        input_path = os.path.join(emotion_path, audio_file)\n",
        "        audio, sr = librosa.load(input_path, sr=None)  # sr=None mantiene la tasa de muestreo original\n",
        "\n",
        "        # Guardar el audio original (opcional)\n",
        "        output_path = os.path.join(output_emotion_path, audio_file)\n",
        "        sf.write(output_path, audio, sr)\n",
        "\n",
        "        # Generar audios aumentados\n",
        "        for i in range(AUGMENTATIONS_PER_FILE):\n",
        "            augmented_audio = augmenter(samples=audio, sample_rate=sr)\n",
        "            output_aug_path = os.path.join(\n",
        "                output_emotion_path,\n",
        "                f\"{os.path.splitext(audio_file)[0]}_aug{i+1}.wav\"\n",
        "            )\n",
        "            sf.write(output_aug_path, augmented_audio, sr)\n",
        "\n",
        "print(\"¡Data augmentation completado! Verifica la carpeta:\", OUTPUT_FOLDER)"
      ],
      "metadata": {
        "id": "0Uhi9FMsCHi-",
        "outputId": "dc777428-b4a9-48c2-b10f-1392aba9ae93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0Uhi9FMsCHi-",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando: Alegria...\n",
            "Procesando: Enojo...\n",
            "Procesando: Miedo...\n",
            "Procesando: Tristeza...\n",
            "Procesando: Neutral...\n",
            "Procesando: Sorpresa...\n",
            "¡Data augmentation completado! Verifica la carpeta: Dataset_Augmented\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f1922021-68ba-444c-bbc7-2a5db79164a9",
      "metadata": {
        "id": "f1922021-68ba-444c-bbc7-2a5db79164a9",
        "outputId": "cfdde341-c8a4-4e60-cecd-1b50398f7fac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Dataset_TFM_recortado/Alegria'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-275686915.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memotion_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset_TFM_recortado/Alegria'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Configuración\n",
        "emotion_labels = ['Alegria', 'Enojo', 'Miedo', 'Neutral', 'Sorpresa', 'Tristeza']\n",
        "dataset_path = 'Dataset_Augmented/'\n",
        "sample_rate = 22050\n",
        "max_duration = 3  # segundos\n",
        "n_mfcc = 40\n",
        "\n",
        "def extract_features(file_path):\n",
        "    audio, sr = librosa.load(file_path, sr=sample_rate, duration=max_duration)\n",
        "    if sr != sample_rate:\n",
        "        audio = librosa.resample(audio, orig_sr=sr, target_sr=sample_rate)\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
        "    return np.mean(mfcc.T, axis=0)  # resultado: (40,)\n",
        "'''\n",
        "def extract_features(file_path):\n",
        "    audio, sr = librosa.load(file_path, sr=sample_rate, duration=max_duration)\n",
        "    if sr != sample_rate:\n",
        "        audio = librosa.resample(audio, orig_sr=sr, target_sr=sample_rate)\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=1)  # 1 sola dimensión por frame\n",
        "    return mfcc.T  # Resultado: (tiempo, 1) → típicamente (1024, 1)\n",
        "\n",
        "max_frames = 1024\n",
        "n_mfcc = 1\n",
        "\n",
        "def extract_features(file_path):\n",
        "    audio, sr = librosa.load(file_path, sr=22050, duration=3.0)\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc).T  # → (T, 1)\n",
        "\n",
        "    # Padding o truncado para que todas tengan exactamente 1024 frames\n",
        "    if mfcc.shape[0] < max_frames:\n",
        "        padding = np.zeros((max_frames - mfcc.shape[0], n_mfcc))\n",
        "        mfcc = np.vstack((mfcc, padding))\n",
        "    else:\n",
        "        mfcc = mfcc[:max_frames, :]\n",
        "\n",
        "    return mfcc  # forma final: (1024, 1)\n",
        "'''\n",
        "\n",
        "\n",
        "X, y = [], []\n",
        "for label in emotion_labels:\n",
        "    folder_path = os.path.join(dataset_path, label)\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith('.wav'):\n",
        "            features = extract_features(os.path.join(folder_path, file))\n",
        "            X.append(features)\n",
        "            y.append(label)\n",
        "\n",
        "X = np.array(X)\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b8f899-47f1-476d-9222-34245e849d99",
      "metadata": {
        "id": "69b8f899-47f1-476d-9222-34245e849d99",
        "outputId": "be8c5612-92fb-4ca1-835c-57ac2926d668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1169, 1024, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15a6526a-0ebb-4a29-acb7-49b586428d7a",
      "metadata": {
        "id": "15a6526a-0ebb-4a29-acb7-49b586428d7a"
      },
      "source": [
        "### Codigo ideal con YAMNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ed3acb8-3b15-43b0-a440-5b11429d0efc",
      "metadata": {
        "id": "4ed3acb8-3b15-43b0-a440-5b11429d0efc"
      },
      "outputs": [],
      "source": [
        "#!pip install librosa tensorflow tensorflow_hub scikit-learn soundfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11a63948-2124-4c66-ace2-d77ab1820d36",
      "metadata": {
        "id": "11a63948-2124-4c66-ace2-d77ab1820d36",
        "outputId": "32875c37-3a39-4c26-b3c7-3c5405509fc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando modelo YAMNet...\n",
            "Extrayendo embeddings de YAMNet...\n",
            "Entrenando clasificador Random Forest...\n",
            "\n",
            "--- Reporte de clasificación ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Alegria       0.43      0.43      0.43        46\n",
            "       Enojo       0.39      0.40      0.39        48\n",
            "       Miedo       0.35      0.34      0.35        50\n",
            "     Neutral       0.22      0.26      0.24        50\n",
            "    Sorpresa       0.50      0.38      0.43        48\n",
            "    Tristeza       0.36      0.37      0.37        51\n",
            "\n",
            "    accuracy                           0.36       293\n",
            "   macro avg       0.37      0.36      0.37       293\n",
            "weighted avg       0.37      0.36      0.37       293\n",
            "\n",
            "\n",
            "--- Matriz de confusión ---\n",
            "[[20 11  1 10  3  1]\n",
            " [ 6 19  7  4  8  4]\n",
            " [ 3  7 17  9  0 14]\n",
            " [ 8  4  7 13  5 13]\n",
            " [ 9  7  3  9 18  2]\n",
            " [ 1  1 13 15  2 19]]\n",
            "✅ Modelo guardado como modelo_emociones_yamnet.pkl\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ✅ Requiere instalación previa:\n",
        "# pip install librosa tensorflow tensorflow_hub scikit-learn soundfile\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import joblib\n",
        "\n",
        "# --- Configuración ---\n",
        "DATASET_PATH = 'Dataset_TFM_recortado'  # Estructura: dataset/Alegria/, dataset/Enojo/, etc.\n",
        "EMOTIONS = ['Alegria', 'Enojo', 'Miedo', 'Neutral', 'Sorpresa', 'Tristeza']\n",
        "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
        "\n",
        "# --- Cargar modelo YAMNet ---\n",
        "print(\"Cargando modelo YAMNet...\")\n",
        "yamnet_model = hub.load(yamnet_model_handle)\n",
        "\n",
        "# --- Función para extraer embeddings ---\n",
        "def extract_yamnet_embedding(file_path):\n",
        "    try:\n",
        "        waveform, sr = librosa.load(file_path, sr=16000)\n",
        "        scores, embeddings, spectrogram = yamnet_model(waveform)\n",
        "        return np.mean(embeddings.numpy(), axis=0)\n",
        "    except Exception as e:\n",
        "        print(f\"Error en {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Extraer características ---\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "print(\"Extrayendo embeddings de YAMNet...\")\n",
        "for emotion in EMOTIONS:\n",
        "    emotion_path = os.path.join(DATASET_PATH, emotion)\n",
        "    for fname in os.listdir(emotion_path):\n",
        "        if fname.endswith(\".wav\"):\n",
        "            file_path = os.path.join(emotion_path, fname)\n",
        "            embedding = extract_yamnet_embedding(file_path)\n",
        "            if embedding is not None:\n",
        "                X.append(embedding)\n",
        "                y.append(emotion)\n",
        "\n",
        "X = np.array(X)\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "# --- División entrenamiento/prueba ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# --- Entrenamiento con Random Forest ---\n",
        "print(\"Entrenando clasificador Random Forest...\")\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# --- Evaluación ---\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"\\n--- Reporte de clasificación ---\")\n",
        "print(classification_report(y_test, y_pred, target_names=EMOTIONS))\n",
        "\n",
        "print(\"\\n--- Matriz de confusión ---\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# --- Validación cruzada (opcional) ---\n",
        "# scores = cross_val_score(clf, X, y, cv=5)\n",
        "# print(f\"Precisión media (cross-validation): {np.mean(scores):.2f}\")\n",
        "\n",
        "# --- Guardar modelo entrenado ---\n",
        "joblib.dump(clf, \"modelo_emociones_yamnet.pkl\")\n",
        "print(\"✅ Modelo guardado como modelo_emociones_yamnet.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00b94417-516f-4ee5-b90c-e5f60f4f3ff1",
      "metadata": {
        "id": "00b94417-516f-4ee5-b90c-e5f60f4f3ff1"
      },
      "source": [
        "### Modelo 1: CNN simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22431599-c4a9-497a-9668-67f5b189f8e7",
      "metadata": {
        "id": "22431599-c4a9-497a-9668-67f5b189f8e7",
        "outputId": "10f32080-a02d-4619-9fa6-a536b1c3742c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\curso\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_14\" is incompatible with the layer: expected axis -1 of input shape to have value 896, but received input with shape (None, 32384)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 1024, 1), dtype=float32)\n  • training=True\n  • mask=None\n  • kwargs=<class 'inspect._empty'>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[75], line 20\u001b[0m\n\u001b[0;32m      7\u001b[0m model_cnn \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m      8\u001b[0m     Conv1D(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m5\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(n_mfcc, \u001b[38;5;241m1\u001b[39m)),\n\u001b[0;32m      9\u001b[0m     MaxPooling1D(\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     Dense(\u001b[38;5;28mlen\u001b[39m(emotion_labels), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m ])\n\u001b[0;32m     19\u001b[0m model_cnn\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 20\u001b[0m model_cnn\u001b[38;5;241m.\u001b[39mfit(X_train_cnn, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test_cnn, y_test))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# --- Predicciones ---\u001b[39;00m\n\u001b[0;32m     23\u001b[0m y_pred_probs \u001b[38;5;241m=\u001b[39m model_cnn\u001b[38;5;241m.\u001b[39mpredict(X_test_cnn)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_14\" is incompatible with the layer: expected axis -1 of input shape to have value 896, but received input with shape (None, 32384)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 1024, 1), dtype=float32)\n  • training=True\n  • mask=None\n  • kwargs=<class 'inspect._empty'>"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "\n",
        "X_train_cnn = X_train[..., np.newaxis]\n",
        "X_test_cnn = X_test[..., np.newaxis]\n",
        "\n",
        "model_cnn = Sequential([\n",
        "    Conv1D(64, 5, activation='relu', input_shape=(n_mfcc, 1)),\n",
        "    MaxPooling1D(2),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    MaxPooling1D(2),\n",
        "    Flatten(),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(emotion_labels), activation='softmax')\n",
        "])\n",
        "\n",
        "model_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_cnn.fit(X_train_cnn, y_train, epochs=50, batch_size=32, validation_data=(X_test_cnn, y_test))\n",
        "\n",
        "# --- Predicciones ---\n",
        "y_pred_probs = model_cnn.predict(X_test_cnn)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# --- Matriz de confusión ---\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=emotion_labels,\n",
        "            yticklabels=emotion_labels)\n",
        "plt.xlabel('Predicción')\n",
        "plt.ylabel('Etiqueta real')\n",
        "plt.title('Matriz de Confusión - CNN Simple')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Reporte de clasificación ---\n",
        "print(\"\\n📋 Clasificación por clase:\")\n",
        "print(classification_report(y_test, y_pred, target_names=emotion_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32385055-64cc-4ebe-a96a-8e4d42ec978d",
      "metadata": {
        "id": "32385055-64cc-4ebe-a96a-8e4d42ec978d",
        "outputId": "d24ac53f-8b89-4ef6-b561-8e87a6aaa808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1169, 40)\n",
            "(1169, 40, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)        # debería ser (N, 40)\n",
        "print(X_train_cnn.shape)    # debería ser (N, 40, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "070971c6-25cd-40b8-8c39-47cf1be98c0b",
      "metadata": {
        "id": "070971c6-25cd-40b8-8c39-47cf1be98c0b"
      },
      "source": [
        "### Modelo 2: LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8e9ff9a-f626-46e2-bc33-c053ae184909",
      "metadata": {
        "id": "a8e9ff9a-f626-46e2-bc33-c053ae184909",
        "outputId": "4f6b5bf4-aa64-4432-ae4b-89a76a372b3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.1747 - loss: 1.8138 - val_accuracy: 0.1706 - val_loss: 1.7857\n",
            "Epoch 2/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1932 - loss: 1.7802 - val_accuracy: 0.1775 - val_loss: 1.7827\n",
            "Epoch 3/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2160 - loss: 1.7771 - val_accuracy: 0.1809 - val_loss: 1.7853\n",
            "Epoch 4/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2222 - loss: 1.7534 - val_accuracy: 0.1775 - val_loss: 1.7763\n",
            "Epoch 5/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2529 - loss: 1.7330 - val_accuracy: 0.1809 - val_loss: 1.7687\n",
            "Epoch 6/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2870 - loss: 1.7213 - val_accuracy: 0.1775 - val_loss: 1.7693\n",
            "Epoch 7/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2647 - loss: 1.7056 - val_accuracy: 0.1809 - val_loss: 1.7706\n",
            "Epoch 8/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2889 - loss: 1.6798 - val_accuracy: 0.1706 - val_loss: 1.7629\n",
            "Epoch 9/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3194 - loss: 1.6208 - val_accuracy: 0.1741 - val_loss: 1.7589\n",
            "Epoch 10/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3344 - loss: 1.6223 - val_accuracy: 0.1911 - val_loss: 1.7597\n",
            "Epoch 11/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3379 - loss: 1.5860 - val_accuracy: 0.2662 - val_loss: 1.7444\n",
            "Epoch 12/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3618 - loss: 1.5593 - val_accuracy: 0.2014 - val_loss: 1.7442\n",
            "Epoch 13/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3529 - loss: 1.5358 - val_accuracy: 0.2048 - val_loss: 1.7384\n",
            "Epoch 14/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3867 - loss: 1.5133 - val_accuracy: 0.2423 - val_loss: 1.7280\n",
            "Epoch 15/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4124 - loss: 1.4724 - val_accuracy: 0.2491 - val_loss: 1.7174\n",
            "Epoch 16/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4523 - loss: 1.4505 - val_accuracy: 0.2253 - val_loss: 1.7486\n",
            "Epoch 17/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4430 - loss: 1.4302 - val_accuracy: 0.2560 - val_loss: 1.7880\n",
            "Epoch 18/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4462 - loss: 1.3906 - val_accuracy: 0.2423 - val_loss: 1.7701\n",
            "Epoch 19/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4907 - loss: 1.3242 - val_accuracy: 0.2526 - val_loss: 1.7961\n",
            "Epoch 20/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4839 - loss: 1.2898 - val_accuracy: 0.2491 - val_loss: 1.8009\n",
            "Epoch 21/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5243 - loss: 1.2310 - val_accuracy: 0.2628 - val_loss: 1.7887\n",
            "Epoch 22/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5284 - loss: 1.2233 - val_accuracy: 0.2730 - val_loss: 1.8043\n",
            "Epoch 23/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5433 - loss: 1.2247 - val_accuracy: 0.2969 - val_loss: 1.8193\n",
            "Epoch 24/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5504 - loss: 1.1492 - val_accuracy: 0.2969 - val_loss: 1.8451\n",
            "Epoch 25/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5683 - loss: 1.1938 - val_accuracy: 0.2935 - val_loss: 1.8626\n",
            "Epoch 26/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5920 - loss: 1.0823 - val_accuracy: 0.2935 - val_loss: 1.8937\n",
            "Epoch 27/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5917 - loss: 1.0914 - val_accuracy: 0.2969 - val_loss: 1.8419\n",
            "Epoch 28/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6082 - loss: 1.0552 - val_accuracy: 0.2867 - val_loss: 1.9648\n",
            "Epoch 29/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5808 - loss: 1.0522 - val_accuracy: 0.2628 - val_loss: 1.9888\n",
            "Epoch 30/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6259 - loss: 0.9884 - val_accuracy: 0.3003 - val_loss: 1.9723\n",
            "Epoch 31/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6248 - loss: 1.0053 - val_accuracy: 0.2799 - val_loss: 2.0206\n",
            "Epoch 32/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6097 - loss: 0.9883 - val_accuracy: 0.2901 - val_loss: 2.0546\n",
            "Epoch 33/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6337 - loss: 0.9203 - val_accuracy: 0.2901 - val_loss: 2.1071\n",
            "Epoch 34/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7067 - loss: 0.8166 - val_accuracy: 0.2969 - val_loss: 2.0857\n",
            "Epoch 35/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6954 - loss: 0.8197 - val_accuracy: 0.3242 - val_loss: 2.1136\n",
            "Epoch 36/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7121 - loss: 0.7725 - val_accuracy: 0.3038 - val_loss: 2.1630\n",
            "Epoch 37/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7066 - loss: 0.8002 - val_accuracy: 0.3242 - val_loss: 2.2075\n",
            "Epoch 38/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7221 - loss: 0.7675 - val_accuracy: 0.3276 - val_loss: 2.2651\n",
            "Epoch 39/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7503 - loss: 0.6841 - val_accuracy: 0.3072 - val_loss: 2.2814\n",
            "Epoch 40/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7394 - loss: 0.7097 - val_accuracy: 0.3072 - val_loss: 2.3221\n",
            "Epoch 41/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7629 - loss: 0.6985 - val_accuracy: 0.2935 - val_loss: 2.4059\n",
            "Epoch 42/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7620 - loss: 0.6807 - val_accuracy: 0.2935 - val_loss: 2.3595\n",
            "Epoch 43/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7669 - loss: 0.6306 - val_accuracy: 0.3140 - val_loss: 2.4681\n",
            "Epoch 44/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7961 - loss: 0.5843 - val_accuracy: 0.3242 - val_loss: 2.4689\n",
            "Epoch 45/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7913 - loss: 0.5827 - val_accuracy: 0.3515 - val_loss: 2.4601\n",
            "Epoch 46/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7938 - loss: 0.5583 - val_accuracy: 0.3140 - val_loss: 2.5468\n",
            "Epoch 47/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8025 - loss: 0.5478 - val_accuracy: 0.3174 - val_loss: 2.5847\n",
            "Epoch 48/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8160 - loss: 0.4987 - val_accuracy: 0.3208 - val_loss: 2.6048\n",
            "Epoch 49/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8195 - loss: 0.4663 - val_accuracy: 0.3208 - val_loss: 2.7800\n",
            "Epoch 50/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8503 - loss: 0.4560 - val_accuracy: 0.3242 - val_loss: 2.7372\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x21513d3db20>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Reshape\n",
        "\n",
        "X_train_lstm = X_train.reshape(-1, 10, 4)\n",
        "X_test_lstm = X_test.reshape(-1, 10, 4)\n",
        "\n",
        "model_lstm = Sequential([\n",
        "    LSTM(64, return_sequences=False, input_shape=(10, 4)),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(len(emotion_labels), activation='softmax')\n",
        "])\n",
        "\n",
        "model_lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_lstm.fit(X_train_lstm, y_train, epochs=50, batch_size=32, validation_data=(X_test_lstm, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11e93f6f-097b-42f9-8d8f-df7b75dc1d1e",
      "metadata": {
        "id": "11e93f6f-097b-42f9-8d8f-df7b75dc1d1e"
      },
      "source": [
        "### Modelo 3: CNN + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df1140c7-4b44-49ea-bdde-e3b03267d344",
      "metadata": {
        "id": "df1140c7-4b44-49ea-bdde-e3b03267d344",
        "outputId": "ca1c4fc7-86f3-444d-e736-2fb3d6d1afe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.1617 - loss: 1.8324 - val_accuracy: 0.1911 - val_loss: 1.7987\n",
            "Epoch 2/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2062 - loss: 1.7800 - val_accuracy: 0.1843 - val_loss: 1.7995\n",
            "Epoch 3/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1777 - loss: 1.7822 - val_accuracy: 0.1536 - val_loss: 1.7891\n",
            "Epoch 4/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2482 - loss: 1.7361 - val_accuracy: 0.2014 - val_loss: 1.7836\n",
            "Epoch 5/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2634 - loss: 1.7127 - val_accuracy: 0.1809 - val_loss: 1.7841\n",
            "Epoch 6/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2578 - loss: 1.6972 - val_accuracy: 0.1945 - val_loss: 1.7801\n",
            "Epoch 7/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3062 - loss: 1.6514 - val_accuracy: 0.2150 - val_loss: 1.7566\n",
            "Epoch 8/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3421 - loss: 1.6098 - val_accuracy: 0.2287 - val_loss: 1.7474\n",
            "Epoch 9/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3040 - loss: 1.6109 - val_accuracy: 0.2321 - val_loss: 1.7438\n",
            "Epoch 10/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3520 - loss: 1.5578 - val_accuracy: 0.2423 - val_loss: 1.7518\n",
            "Epoch 11/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3770 - loss: 1.5414 - val_accuracy: 0.2218 - val_loss: 1.7608\n",
            "Epoch 12/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4092 - loss: 1.4750 - val_accuracy: 0.2355 - val_loss: 1.7671\n",
            "Epoch 13/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4572 - loss: 1.3871 - val_accuracy: 0.2287 - val_loss: 1.7581\n",
            "Epoch 14/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4289 - loss: 1.4411 - val_accuracy: 0.2218 - val_loss: 1.7896\n",
            "Epoch 15/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4537 - loss: 1.3997 - val_accuracy: 0.2662 - val_loss: 1.8067\n",
            "Epoch 16/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4556 - loss: 1.3709 - val_accuracy: 0.2491 - val_loss: 1.7580\n",
            "Epoch 17/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4947 - loss: 1.2845 - val_accuracy: 0.2389 - val_loss: 1.7940\n",
            "Epoch 18/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5327 - loss: 1.2368 - val_accuracy: 0.2253 - val_loss: 1.8089\n",
            "Epoch 19/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5457 - loss: 1.2149 - val_accuracy: 0.2594 - val_loss: 1.8979\n",
            "Epoch 20/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5551 - loss: 1.1524 - val_accuracy: 0.2560 - val_loss: 1.8744\n",
            "Epoch 21/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5547 - loss: 1.1370 - val_accuracy: 0.2594 - val_loss: 1.8271\n",
            "Epoch 22/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5709 - loss: 1.0839 - val_accuracy: 0.2423 - val_loss: 1.9848\n",
            "Epoch 23/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6028 - loss: 1.0499 - val_accuracy: 0.2696 - val_loss: 1.9532\n",
            "Epoch 24/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6212 - loss: 0.9969 - val_accuracy: 0.2935 - val_loss: 1.9145\n",
            "Epoch 25/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6211 - loss: 0.9569 - val_accuracy: 0.2696 - val_loss: 2.0539\n",
            "Epoch 26/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6536 - loss: 0.8889 - val_accuracy: 0.2628 - val_loss: 1.9722\n",
            "Epoch 27/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6751 - loss: 0.8709 - val_accuracy: 0.2594 - val_loss: 2.0513\n",
            "Epoch 28/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6978 - loss: 0.8454 - val_accuracy: 0.3072 - val_loss: 2.0210\n",
            "Epoch 29/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7183 - loss: 0.7859 - val_accuracy: 0.3072 - val_loss: 2.1003\n",
            "Epoch 30/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7031 - loss: 0.8018 - val_accuracy: 0.3174 - val_loss: 2.0618\n",
            "Epoch 31/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7423 - loss: 0.7052 - val_accuracy: 0.3106 - val_loss: 2.1157\n",
            "Epoch 32/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7467 - loss: 0.6997 - val_accuracy: 0.3276 - val_loss: 2.0755\n",
            "Epoch 33/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7838 - loss: 0.6525 - val_accuracy: 0.3174 - val_loss: 2.2179\n",
            "Epoch 34/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7938 - loss: 0.6026 - val_accuracy: 0.2833 - val_loss: 2.2373\n",
            "Epoch 35/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7858 - loss: 0.6002 - val_accuracy: 0.3345 - val_loss: 2.2611\n",
            "Epoch 36/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7956 - loss: 0.5687 - val_accuracy: 0.3311 - val_loss: 2.2667\n",
            "Epoch 37/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7969 - loss: 0.5256 - val_accuracy: 0.3208 - val_loss: 2.4851\n",
            "Epoch 38/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8371 - loss: 0.4791 - val_accuracy: 0.3311 - val_loss: 2.4677\n",
            "Epoch 39/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8609 - loss: 0.4083 - val_accuracy: 0.3276 - val_loss: 2.4371\n",
            "Epoch 40/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8389 - loss: 0.4776 - val_accuracy: 0.3038 - val_loss: 2.5632\n",
            "Epoch 41/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8677 - loss: 0.3857 - val_accuracy: 0.3174 - val_loss: 2.5504\n",
            "Epoch 42/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8786 - loss: 0.3543 - val_accuracy: 0.3413 - val_loss: 2.6648\n",
            "Epoch 43/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8893 - loss: 0.3471 - val_accuracy: 0.3481 - val_loss: 2.6555\n",
            "Epoch 44/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8965 - loss: 0.3267 - val_accuracy: 0.3276 - val_loss: 2.7329\n",
            "Epoch 45/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9134 - loss: 0.2746 - val_accuracy: 0.3345 - val_loss: 2.6911\n",
            "Epoch 46/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9067 - loss: 0.2713 - val_accuracy: 0.3208 - val_loss: 2.8730\n",
            "Epoch 47/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9189 - loss: 0.2380 - val_accuracy: 0.3276 - val_loss: 3.0151\n",
            "Epoch 48/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9280 - loss: 0.2331 - val_accuracy: 0.3379 - val_loss: 2.9535\n",
            "Epoch 49/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9119 - loss: 0.2648 - val_accuracy: 0.3276 - val_loss: 2.9417\n",
            "Epoch 50/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9282 - loss: 0.2328 - val_accuracy: 0.3208 - val_loss: 3.1453\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x215092aab10>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
        "\n",
        "X_train_cnn_lstm = X_train.reshape(-1, 10, 4)\n",
        "X_test_cnn_lstm = X_test.reshape(-1, 10, 4)\n",
        "\n",
        "model_cnn_lstm = Sequential([\n",
        "    Conv1D(64, 3, activation='relu', input_shape=(10, 4)),\n",
        "    MaxPooling1D(2),\n",
        "    LSTM(64),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(len(emotion_labels), activation='softmax')\n",
        "])\n",
        "\n",
        "model_cnn_lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_cnn_lstm.fit(X_train_cnn_lstm, y_train, epochs=50, batch_size=32, validation_data=(X_test_cnn_lstm, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "080d0a95-b345-40c0-a812-72cc1cfe3c56",
      "metadata": {
        "id": "080d0a95-b345-40c0-a812-72cc1cfe3c56"
      },
      "source": [
        "### Modelo 4: Transfer Learning con YAMNet (TensorFlow Hub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e81a1b3c-0362-4106-a3a1-b4ed2800e337",
      "metadata": {
        "id": "e81a1b3c-0362-4106-a3a1-b4ed2800e337",
        "outputId": "bbfc3633-e448-4929-d413-3cd5d1df33e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precisión: 0.310580204778157\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
        "yamnet_model = hub.load(yamnet_model_handle)\n",
        "\n",
        "def extract_yamnet_embeddings(file_path):\n",
        "    waveform, sr = librosa.load(file_path, sr=16000)\n",
        "    scores, embeddings, _ = yamnet_model(waveform)\n",
        "    return np.mean(embeddings.numpy(), axis=0)\n",
        "\n",
        "X_yam, y_yam = [], []\n",
        "for label in emotion_labels:\n",
        "    folder_path = os.path.join(dataset_path, label)\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith('.wav'):\n",
        "            emb = extract_yamnet_embeddings(os.path.join(folder_path, file))\n",
        "            X_yam.append(emb)\n",
        "            y_yam.append(label)\n",
        "\n",
        "X_yam = np.array(X_yam)\n",
        "y_yam = LabelEncoder().fit_transform(y_yam)\n",
        "X_train_yam, X_test_yam, y_train_yam, y_test_yam = train_test_split(X_yam, y_yam, test_size=0.2)\n",
        "\n",
        "# Clasificador final\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100)\n",
        "rf_model.fit(X_train_yam, y_train_yam)\n",
        "print(\"Precisión:\", rf_model.score(X_test_yam, y_test_yam))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0012a3c4-8779-4f4b-b41e-06cc7cfcacc9",
      "metadata": {
        "id": "0012a3c4-8779-4f4b-b41e-06cc7cfcacc9",
        "outputId": "bf79e7c2-5bec-48d5-e624-2175fd159624"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model_cl' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[47], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_cnn\u001b[38;5;241m.\u001b[39mevaluate(X_test_cnn, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_lstm\u001b[38;5;241m.\u001b[39mevaluate(X_test_lstm, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN+LSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_cl\u001b[38;5;241m.\u001b[39mevaluate(X_test_cl, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYAMNet\u001b[39m\u001b[38;5;124m\"\u001b[39m: rf\u001b[38;5;241m.\u001b[39mscore(X_test_yam, y_test_yam)\n\u001b[0;32m      6\u001b[0m }\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model, acc \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model_cl' is not defined"
          ]
        }
      ],
      "source": [
        "results = {\n",
        "    \"CNN\": model_cnn.evaluate(X_test_cnn, y_test, verbose=0)[1],\n",
        "    \"LSTM\": model_lstm.evaluate(X_test_lstm, y_test, verbose=0)[1],\n",
        "    \"CNN+LSTM\": model_cl.evaluate(X_test_cl, y_test, verbose=0)[1],\n",
        "    \"YAMNet\": rf.score(X_test_yam, y_test_yam)\n",
        "}\n",
        "\n",
        "for model, acc in results.items():\n",
        "    print(f\"{model}: {acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9295e28f-9814-4353-a25e-cc9cceab26fa",
      "metadata": {
        "id": "9295e28f-9814-4353-a25e-cc9cceab26fa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3a06044-f7af-41da-8219-2d677a070efb",
      "metadata": {
        "id": "a3a06044-f7af-41da-8219-2d677a070efb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8af36d21-83d8-44b6-888b-709440a6c722",
      "metadata": {
        "id": "8af36d21-83d8-44b6-888b-709440a6c722"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5463c15-f06b-449e-8d6c-3a6b3c39ff0c",
      "metadata": {
        "id": "b5463c15-f06b-449e-8d6c-3a6b3c39ff0c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}